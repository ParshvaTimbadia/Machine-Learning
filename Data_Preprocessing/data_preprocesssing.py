# -*- coding: utf-8 -*-
"""Data_Preprocesssing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eWkLf_Pp80ORTbO-GkkWExpjJwOuT3SD

**Importing the Libraries**
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""**Import DataSet**"""

dataset= pd.read_csv("Data.csv")
x= dataset.iloc[:, :-1].values
y= dataset.iloc[:, -1].values

print (x)
print(y)

"""**Calculating the Missing Data**"""

from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit(x[:, 1:3])
x[:, 1:3] =imp_mean.transform(x[:, 1:3])


print (x)

"""**Encoding Independent Variable**"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer([('encoder', OneHotEncoder(), [0])], remainder='passthrough')
x = np.array(ct.fit_transform(x), dtype = np.str)

print (x)

#https://towardsdatascience.com/columntransformer-in-scikit-for-labelencoding-and-onehotencoding-in-machine-learning-c6255952731b
#Use this link for better uniderstanding.

"""**Encoding Dependent Variable**"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y= le.fit_transform(y)

print (y)

"""**Feature Scaling**"""

#What is Feature Scaling and Why ?
# Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.

# Example: If an algorithm is not using feature scaling method then it can consider the value 3000 meter to be greater than 5 km but thatâ€™s actually not true and in this case, the algorithm will give wrong predictions. So, we use Feature Scaling to bring all values to same magnitudes and thus, tackle this issue.

# There are to ways to do Feature Scaling. 1> Standardisation and anothier is 2> Normalisation. 

# Here we are implementing Standardization as it is a bit more efficient. 

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x = sc.fit_transform(x)

print (x)

"""**Split Dataset into Test Set and Training Set**"""

from sklearn.model_selection import train_test_split
x_train, y_train , x_test, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

print(x_train)
print(y_train)
print(x_test)
print(y_test)